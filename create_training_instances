ls ./shards/ | xargs -n 1 -P 2 -I {} python3.7 ./create_pretraining_data.py --input_file=./shards/{} --output_file=pretraining_data/{}.tfrecord --vocab_file=vocab.txt --do_lower_case=True --max_predictions_per_seq=20 --max_seq_length=128 --masked_lm_prob=0.15 --random_seed=34 --dupe_factor=5

python3.7 ./create_pretraining_data.py --input_file=./shards/shard_0000 --output_file=pretraining_data/shard_0000.tfrecord --vocab_file=vocab.txt --do_lower_case=True --max_predictions_per_seq=10 --max_seq_length=32 --masked_lm_prob=0.15 --random_seed=34 --dupe_factor=5



TRAIN_BATCH_SIZE = 32 #@param {type:"integer"}
MAX_PREDICTIONS = 10 #@param {type:"integer"}
MAX_SEQ_LENGTH = 32 #@param {type:"integer"}
MASKED_LM_PROB = 0.15 #@param

# Training procedure config
EVAL_BATCH_SIZE = 16
LEARNING_RATE = 2e-5
TRAIN_STEPS = 1000000 #@param {type:"integer"}
SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:"integer"}
NUM_TPU_CORES = None

model_fn = model_fn_builder(
      bert_config=bert_config,
      init_checkpoint=INIT_CHECKPOINT,
      learning_rate=LEARNING_RATE,
      num_train_steps=TRAIN_STEPS,
      num_warmup_steps=10,
      use_tpu=USE_TPU,
      use_one_hot_embeddings=True)


run_config = tf.contrib.tpu.RunConfig(
    cluster=None,
    model_dir=BERT_GCS_DIR,
    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,
    tpu_config=tf.contrib.tpu.TPUConfig(
        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,
        num_shards=4,
        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))

estimator = tf.contrib.tpu.TPUEstimator(
    use_tpu=USE_TPU,
    model_fn=model_fn,
    config=run_config,
    train_batch_size=TRAIN_BATCH_SIZE,
    eval_batch_size=EVAL_BATCH_SIZE)
  
train_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=MAX_SEQ_LENGTH,
        max_predictions_per_seq=MAX_PREDICTIONS,
        is_training=True)


bert_base_config = {
  "attention_probs_dropout_prob": 0.1, 
  "directionality": "bidi", 
  "hidden_act": "gelu", 
  "hidden_dropout_prob": 0.1, 
  "hidden_size": 384, 
  "initializer_range": 0.02, 
  "intermediate_size": 1536, 
  "max_position_embeddings": 512, 
  "num_attention_heads": 6, 
  "num_hidden_layers": 6, 
  "pooler_fc_size": 384, 
  "pooler_num_attention_heads": 6, 
  "pooler_num_fc_layers": 3, 
  "pooler_size_per_head": 6, 
  "pooler_type": "first_token_transform", 
  "type_vocab_size": 2, 
  "vocab_size": VOC_SIZE
}